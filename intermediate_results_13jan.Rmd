---
title: "Intermediate results of GS models"
author: "Quentin D. Read"
date: "1/13/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, echo = FALSE)
```

I am in the process of running 25 iterations of the GS model for each of the 13 traits, separately for each of the 3 crop cycles. 39 combinations of trait by crop cycle, all repeated 25 times, is a little less than 1000 runs. Each run is 5-fold cross-validation and each fold runs 5 different GS models so that's a lot of computations. For now, I am using "default" settings for all the models. Everything is copied over from Marcus' code. I am not varying marker density (using 100% of the markers every time) and I am not varying training size (20% of the data are held back as a test set with each fold of the CV every time).

It is running on SciNet at the moment and will take a couple of days. I pulled out whatever initial results are available after half a day. I would say that the results are not terrible but also not great. For example, I worked out that we would expect a model's coincidence index (CI) to be about 17% if genotypes are selected randomly. So if the model performs well, it should give us CI substantially higher than the "null" CI of 17%. But unfortunately the CI values are not much higher than 20%, rarely even as high as 30%.

In this document I'll give a few graphs and tables of results and also list a few issues for potential discussion.

# Results

In these results, I will give three metrics of accuracy:

- `r` is the correlation between observed and predicted values (want to maximize)
- `CI` is the coincidence index between the top 20% genotypes as predicted by the model, and the top 20% if selected randomly (want to maximize, it would be 20% if the model was no better than random)
- `RMSE` is the root mean squared prediction error (want to minimize)

```{r read data}
library(tidyverse)
library(gt)

metrics <- read_csv('project/output/metrics_13jan2022.csv')

physical_traits <- c("stkwt_kg", "diam", "Brix", "Fiber", "Pol", "Sucrose", "Purity", "stalk_ha")
economic_traits <- c("TCH", "TRS", "CRS", "TSH", "EI")

theme_set(theme_bw() +
            theme(strip.background = element_blank(),
                  legend.position = 'bottom'))

fill_scale <- scale_fill_brewer(palette = 'Dark2')
null_line <- geom_hline(yintercept = 0.167, size = 1.2, linetype = 'dashed')
```

## Performance of the models for each trait and crop cycle

Here I plot the CI. I put a horizontal line at the null expectation which indicates that the model does no better than random. Different plots are shown for each crop cycle. To me it looks like the models perform better for the first ratoon than for either plant cane or second ratoon. The traits that seem to be better predicted are diameter, stalk weight, and tonnes of cane per hectare. All those are basically related to the physical structure of the cane and not anything to do with sugar content itself -- those traits are not very well predicted.

```{r}
ggplot(metrics %>% filter(crop_cycle %in% 'Plant_Cane_2017'), aes(x = trait, y = CI, fill = model, group = interaction(trait, model))) +
  geom_boxplot(position = 'dodge') +
  ggtitle('Model performance comparison by trait', 'Crop cycle: plant cane (2017)') +
  fill_scale + null_line

ggplot(metrics %>% filter(crop_cycle %in% 'Ratoon_1_2018'), aes(x = trait, y = CI, fill = model, group = interaction(trait, model))) +
  geom_boxplot(position = 'dodge') +
  ggtitle('Model performance comparison by trait', 'Crop cycle: first ratoon (2018)') +
  fill_scale + null_line

ggplot(metrics %>% filter(crop_cycle %in% 'Plant_Cane_2017'), aes(x = trait, y = CI, fill = model, group = interaction(trait, model))) +
  geom_boxplot(position = 'dodge') +
  ggtitle('Model performance comparison by trait', 'Crop cycle: second ratoon (2019)') +
  fill_scale + null_line
```

## Ranking the models

Here are tables averaging the prediction accuracy metrics by crop cycle, and then by trait. For each case, the models with better performance (highest `r`, highest `CI`, or lowest `RMSE`) are highlighted. Overall it looks like BGLR performs best but for some individual traits other models do better. The metrics generally agree.

### Tables by crop cycle

```{r, results = 'asis'}
table_list <- metrics %>%
  group_by(crop_cycle, model) %>%
  summarize(across(c(r, CI, RMSE), mean)) %>%
  group_split(.keep = FALSE)

walk2(table_list, c('Plant Cane 2017', 'First Ratoon 2018', 'Second Ratoon 2019'), 
     ~ print(gt(data = .x) %>%
       tab_header(title = .y) %>%     
       fmt_number(columns = c(r, CI, RMSE), decimals = 3) %>%
       data_color(
         columns = c(r, CI),
         colors = c('black','black','black','black','red'),
         apply_to = c("text")
       ) %>%
       data_color(
         columns = c(RMSE),
         colors = c('red', 'black','black','black','black'),
         apply_to = c("text")
       ))
)
```

### Tables by trait

```{r, results = 'asis'}
table_list <- metrics %>%
  group_by(trait, model) %>%
  summarize(across(c(r, CI, RMSE), mean)) %>%
  group_split()

walk(table_list, 
     ~ print(gt(data = .x %>% select(-trait)) %>%
       tab_header(title = .x$trait[1]) %>%     
       fmt_number(columns = c(r, CI, RMSE), decimals = 3) %>%
       data_color(
         columns = c(r, CI),
         colors = c('black','black','black','black','red'),
         apply_to = c("text")
       ) %>%
       data_color(
         columns = c(RMSE),
         colors = c('red', 'black','black','black','black'),
         apply_to = c("text")
       ))
)
```

# Issues for potential discussion

- ADE model is giving very poor performance. I am a little bit skeptical of the `sommer` package that we are using to implement that model. Is there a different way to do it? In principle the `BGLR` package that we are already using could be adapted to include A, D, and E random effects so maybe we should use that because `BGLR` seems to perform well as we're currently using it.
- Marcus' script varied the training size but that was done within the existing cross-validation. So for example if training size was set to 50%, he would take the 80% of the data for training for each fold of 5-fold CV, then further narrow that down by 50%. I haven't seen that done so I was curious why it was done that way.
- The SVM and RF models don't perform that well but there are many potential ways to fit those models. The existing script just tries one. I would say it's a good idea to try more than one method of SVM and RF, and also try different tuning parameters within each method. Or maybe the methods in Marcus' script are considered best? I don't know.
