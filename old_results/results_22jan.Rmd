---
title: "Results of GS models, version 2"
author: "Quentin D. Read"
date: "1/22/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, echo = FALSE)
```

**edit 24 January**: Fixed bug where diameter was left out for ratoon crops. Also make plots of the correlation coefficient `r`, and set axes to be the same across all plots of each metric for better comparing.

Here is another set of preliminary results, after making changes discussed in the group meeting. The changes are as follows:

- I fit the BLUPs for each genotype for each trait. I did this with the `lmer()` function where crop cycle is a fixed effect, and genotype, row, and column was a random effect (the column BLUPs were zeroed out by the model so there is no column effect but there is a row effect).
- I added some additional models. See discussion below.
- I only ran 5 iterations of the five-fold cross-validation per combination of trait and crop cycle so that I could get a decent amount of data to plot for each combination (I can run more later if we decide it is worthwhile to do so).

## Additional models used

I added BayesA and BayesB as implemented by the BGLR package.

I did a very unscientific attempt at looking at whether varying the hyperparameters of the BayesA and BayesB prior distributions would make any difference. It looks like the default hyperparameters work as well as any other choice, at least for the few traits I haphazardly tried.

I also unsystematically looked at a couple of traits to see if altering the tuning parameters of the SVM and RF models makes any difference. I saw very little difference so I decided not to play with the default tuning parameters when running all the traits. However I did see that changing the kernel of the SVM from a radial basis to a linear or sigmoid kernel sometimes did improve the prediction accuracy somewhat. So I decided to fit SVMs with those three different kernels for all the traits. 

So these results include the following:

- 13 traits
- 3 crop cycles (plant cane, ratoon 1, ratoon 2)
- 9 models (rrBLUP, ADE, RKHS, BayesA, BayesB, SVM radial kernel, SVM linear kernel, SVM sigmoid kernel, and random forest)
- 5-fold cross validation for all combinations above
- 5 iterations of the cross-validation for every combination

Following are the results and any outstanding issues for discussion I could think of.

# Results

In these results, I will give three metrics of accuracy:

- `r` is the correlation between observed and predicted values (we want to maximize this)
- `CI` is the coincidence index between the top 20% genotypes as predicted by the model, and the top 20% if selected randomly (we want to maximize this. It would be ~17% if the model was no better than random)
- `RMSE` is the root mean squared prediction error from all the out-of-sample predictions in the cross-validation (we want to minimize this)

I plotted the first two.

```{r read data}
library(tidyverse)
library(gt)

metrics <- read_csv('project/output/all_metrics.csv')

physical_traits <- c("stkwt_kg", "diam", "Brix", "Fiber", "Pol", "Sucrose", "Purity", "stalk_ha")
economic_traits <- c("TCH", "TRS", "CRS", "TSH", "EI")

theme_set(theme_bw() +
            theme(strip.background = element_blank(),
                  legend.position = 'bottom'))

fill_scale <- scale_fill_brewer(palette = 'Dark2')
null_line <- geom_hline(yintercept = 0.167, size = 1.2, linetype = 'dashed')
null_line2 <- geom_hline(yintercept = 0, size = 1.2, linetype = 'dashed')
```

## Performance of the models for each trait and crop cycle

### Coincidence Index (CI)

Here I plot the CI. I put a horizontal line at the null expectation which indicates that the model does no better than random. I separated out the plots by crop cycle and by physical vs. economic traits. The traits that seem to be better predicted are diameter (this was only measured for plant cane in 2017 and not afterward), stalks per hectare, and stalk weight. All those are basically related to the physical structure of the cane and not anything to do with sugar content itself -- the sugar content traits are not very well predicted.

### Physical traits

```{r}
ci_y_scale <- scale_y_continuous(limits = range(metrics$CI))

ggplot(metrics %>% filter(crop_cycle %in% 'PlantCane', trait %in% physical_traits), 
       aes(x = trait, y = CI, fill = model, group = interaction(trait, model))) +
  geom_boxplot(position = 'dodge') +
  ggtitle('CI: physical traits', 'Crop cycle: plant cane (2017)') +
  fill_scale + null_line + ci_y_scale

ggplot(metrics %>% filter(crop_cycle %in% 'Ratoon1', trait %in% physical_traits),
       aes(x = trait, y = CI, fill = model, group = interaction(trait, model))) +
  geom_boxplot(position = 'dodge') +
  ggtitle('CI: physical traits', 'Crop cycle: first ratoon (2018)') +
  fill_scale + null_line + ci_y_scale

ggplot(metrics %>% filter(crop_cycle %in% 'Ratoon2', trait %in% physical_traits), 
       aes(x = trait, y = CI, fill = model, group = interaction(trait, model))) +
  geom_boxplot(position = 'dodge') +
  ggtitle('CI: physical traits', 'Crop cycle: second ratoon (2019)') +
  fill_scale + null_line + ci_y_scale
```

### Economic traits

Overall the CI for the economic traits is lower than for the physical traits with essentially no values above 0.3 for any trait across all crop cycles.

```{r}
ggplot(metrics %>% filter(crop_cycle %in% 'PlantCane', trait %in% economic_traits), 
       aes(x = trait, y = CI, fill = model, group = interaction(trait, model))) +
  geom_boxplot(position = 'dodge') +
  ggtitle('CI: economic traits', 'Crop cycle: plant cane (2017)') +
  fill_scale + null_line + ci_y_scale

ggplot(metrics %>% filter(crop_cycle %in% 'Ratoon1', trait %in% economic_traits),
       aes(x = trait, y = CI, fill = model, group = interaction(trait, model))) +
  geom_boxplot(position = 'dodge') +
  ggtitle('CI: economic traits', 'Crop cycle: first ratoon (2018)') +
  fill_scale + null_line + ci_y_scale

ggplot(metrics %>% filter(crop_cycle %in% 'Ratoon2', trait %in% economic_traits), 
       aes(x = trait, y = CI, fill = model, group = interaction(trait, model))) +
  geom_boxplot(position = 'dodge') +
  ggtitle('CI: economic traits', 'Crop cycle: second ratoon (2019)') +
  fill_scale + null_line + ci_y_scale
```

## Correlation coefficient (r)

The following plots show a different metric of model performance: the correlation coefficient `r` between the observed and predicted values. In this case, the expectation for a completely random prediction would be $r = 0$ so I put a dashed line at zero for these plots. Higher correlation means better accuracy; negative means even worse than random.

### Physical traits

We see basically the same patterns as with CI, but with less variability among the different runs (iterations).

```{r}
r_y_scale <- scale_y_continuous(limits = range(metrics$r))

ggplot(metrics %>% filter(crop_cycle %in% 'PlantCane', trait %in% physical_traits), 
       aes(x = trait, y = r, fill = model, group = interaction(trait, model))) +
  geom_boxplot(position = 'dodge') +
  ggtitle('Observed-predicted correlation: physical traits', 'Crop cycle: plant cane (2017)') +
  fill_scale + null_line2 + r_y_scale

ggplot(metrics %>% filter(crop_cycle %in% 'Ratoon1', trait %in% physical_traits),
       aes(x = trait, y = r, fill = model, group = interaction(trait, model))) +
  geom_boxplot(position = 'dodge') +
  ggtitle('Observed-predicted correlation: physical traits', 'Crop cycle: first ratoon (2018)') +
  fill_scale + null_line2 + r_y_scale

ggplot(metrics %>% filter(crop_cycle %in% 'Ratoon2', trait %in% physical_traits), 
       aes(x = trait, y = r, fill = model, group = interaction(trait, model))) +
  geom_boxplot(position = 'dodge') +
  ggtitle('Observed-predicted correlation: physical traits', 'Crop cycle: second ratoon (2019)') +
  fill_scale + null_line2 + r_y_scale
```

### Economic traits

```{r}
ggplot(metrics %>% filter(crop_cycle %in% 'PlantCane', trait %in% economic_traits), 
       aes(x = trait, y = r, fill = model, group = interaction(trait, model))) +
  geom_boxplot(position = 'dodge') +
  ggtitle('Observed-predicted correlation: economic traits', 'Crop cycle: plant cane (2017)') +
  fill_scale + null_line2 + r_y_scale

ggplot(metrics %>% filter(crop_cycle %in% 'Ratoon1', trait %in% economic_traits),
       aes(x = trait, y = r, fill = model, group = interaction(trait, model))) +
  geom_boxplot(position = 'dodge') +
  ggtitle('Observed-predicted correlation: economic traits', 'Crop cycle: first ratoon (2018)') +
  fill_scale + null_line2 + r_y_scale

ggplot(metrics %>% filter(crop_cycle %in% 'Ratoon2', trait %in% economic_traits), 
       aes(x = trait, y = r, fill = model, group = interaction(trait, model))) +
  geom_boxplot(position = 'dodge') +
  ggtitle('Observed-predicted correlation: economic traits', 'Crop cycle: second ratoon (2019)') +
  fill_scale + null_line2 + r_y_scale
```

## Ranking the models

Here are tables averaging the prediction accuracy metrics by crop cycle, and then by trait. For each case, the models with better performance (highest `r`, highest `CI`, or lowest `RMSE`) are highlighted, where darker red color indicates better performance. 

Overall it looks like RKHS and the Bayesian models perform best, and SVM tends to do pretty well too, but for some individual traits other models do better. The metrics generally agree.

### Tables by crop cycle

```{r, results = 'asis'}
table_list <- metrics %>%
  group_by(crop_cycle, model) %>%
  summarize(across(c(r, CI, RMSE), mean)) %>%
  group_split(.keep = FALSE)

color_vec <- rep(c('black','red'), c(7, 2))

walk2(table_list, c('Plant Cane 2017', 'First Ratoon 2018', 'Second Ratoon 2019'), 
     ~ print(gt(data = .x) %>%
       tab_header(title = .y) %>%     
       fmt_number(columns = c(r, CI, RMSE), decimals = 3) %>%
       data_color(
         columns = c(r, CI),
         colors = scales::col_bin(palette = 'Reds', domain = NULL)
       ) %>%
       data_color(
         columns = c(RMSE),
         colors = scales::col_bin(palette = 'Reds', reverse = TRUE, domain = NULL)
       ))
)
```

### Tables by trait

```{r, results = 'asis'}
table_list <- metrics %>%
  group_by(trait, model) %>%
  summarize(across(c(r, CI, RMSE), mean)) %>%
  group_split()

walk(table_list, 
     ~ print(gt(data = .x %>% select(-trait)) %>%
       tab_header(title = .x$trait[1]) %>%     
       fmt_number(columns = c(r, CI, RMSE), decimals = 3) %>%
       data_color(
         columns = c(r, CI),
         colors = scales::col_bin(palette = 'Reds', domain = NULL)
       ) %>%
       data_color(
         columns = c(RMSE),
         colors = scales::col_bin(palette = 'Reds', domain = NULL, reverse = TRUE)
       ))
)
```

# Issues for potential discussion

- In the previous preliminary results I shared, for some reason the predictions were better for ratoon 1 than for the other two crop cycles. I think this was just a chance result, because now we see the predictions are roughly the same quality for all the crop cycles.
- The prediction accuracy is not really that different between the different models. It clearly is much more a function of what trait is being predicted. If the trait has more signal from the genotype, all the models do better.
- Occasionally, for some traits there will be one or two models that perform substantially worse, but there is never any case where one model does substantially better than all the others.
- The RKHS, BayesA, and BayesB seem to outperform the other models slightly. This is probably because Gustavo de los Campos is a genius and also because the models implemented in BGLR are somehow optimized for genomic selection in graminoid crops, maybe? That said, the SVM models are not far behind.
- Using a linear kernel for the SVM looks like a better choice in most cases, which makes sense to me because it is probably simpler and less likely to overfit.
